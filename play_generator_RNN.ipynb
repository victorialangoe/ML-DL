{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Play Generator\n",
    "Create a play using RNN\n",
    "- Show the RNN an example of what we want it to recreate and it will learn how to write a version of it on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train the model on a bunch of sequences of texts from the play \"Romeo and Juliet\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file(\"shakespeare.txt\",\"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read and then decode for py2 compat\n",
    "text = open(path_to_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
    "print(\"Length of text: {} characters\".format(len(text))) # result same as downloaded as you can see over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The first 250 characters in the text\n",
    "print(text[:250]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode each unique character as a different integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "# Create a mapping from unique characters to indices\n",
    "char2idx = {u:i for i,u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def text_to_int(text):\n",
    "    return np.array([char2idx[c] for c in text])\n",
    "\n",
    "text_as_int = text_to_int(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  First Citizen\n",
      "Encoded:  [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "print(\"Text: \", text[:13])\n",
    "print(\"Encoded: \", text_to_int(text[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen\n"
     ]
    }
   ],
   "source": [
    "def int_to_text(ints):\n",
    "    try:\n",
    "        ints = ints.numpy()\n",
    "    except:\n",
    "        pass\n",
    "    return \"\".join(idx2char[ints])\n",
    "print(int_to_text(text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a training example\n",
    "We need to split our text data from above into many shorter sequences that we can pass to the model as training example.\n",
    "\n",
    "The training example will use a *seq_length* sequence as input and a *seq_length* sequence as the output where the sequence is the original sequence shifted one letter to the right. Example:\n",
    "- Input: Hell\n",
    "- Output: ello\n",
    "We could then splice it on the last index and then see that it predicts \"o\" as the last char for the actual word, \"Hello\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100 # length of sequence for a training example\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the batch method to turn this stream of characters into batches of desired length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk): # for the example: hello\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "\n",
      "OUTPUT\n",
      "irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you \n",
      "\n",
      "OUTPUT\n",
      "re all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n"
     ]
    }
   ],
   "source": [
    "for x,y in dataset.take(2):\n",
    "    print(\"\\n\\nEXAMPLE\\n\")\n",
    "    print(\"INPUT\")\n",
    "    print(int_to_text(x))\n",
    "    print(\"\\nOUTPUT\")\n",
    "    print(int_to_text(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab) # number of unique characters\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           16640     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5330241 (20.33 MB)\n",
      "Trainable params: 5330241 (20.33 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer=\"glorot_uniform\"),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "tf.Tensor(\n",
      "[[[ 1.13420701e-03  9.08431772e-04 -2.94142449e-03 ... -2.29606964e-03\n",
      "    2.45451462e-03 -2.10238784e-03]\n",
      "  [ 1.20507274e-02 -9.85535327e-04 -2.11550016e-03 ... -2.41221613e-04\n",
      "   -5.53662842e-03 -1.95679744e-03]\n",
      "  [ 1.03269797e-02  2.05006078e-03 -5.27716102e-03 ...  3.19400779e-03\n",
      "    5.92108467e-04 -1.49707869e-03]\n",
      "  ...\n",
      "  [ 4.78191348e-03  5.79812890e-03  5.60208131e-03 ... -1.06012020e-02\n",
      "    5.12998784e-03 -4.86677838e-03]\n",
      "  [ 3.94266006e-03  6.98575098e-03  7.62356911e-04 ... -6.14348520e-03\n",
      "    9.28916316e-03 -2.48014834e-03]\n",
      "  [ 5.24542946e-03  5.92864258e-03 -3.19685671e-04 ... -4.92570456e-03\n",
      "    1.08414590e-02 -2.57315871e-04]]\n",
      "\n",
      " [[ 5.22475736e-03  9.99243231e-04 -1.61054346e-03 ...  5.14486246e-03\n",
      "    1.67520822e-03 -7.39743561e-03]\n",
      "  [ 5.20211877e-03  7.48357095e-04  4.01893398e-03 ...  1.52268517e-03\n",
      "    8.72902689e-04 -1.09827518e-02]\n",
      "  [ 4.74802777e-03  2.79178284e-03  2.05213437e-05 ...  3.50705162e-03\n",
      "    6.43235818e-03 -6.83996826e-03]\n",
      "  ...\n",
      "  [-2.73815356e-04  8.11831094e-03 -7.64850806e-03 ... -2.34466139e-03\n",
      "    4.25252691e-03 -6.56072376e-03]\n",
      "  [-7.62192649e-05  7.28934910e-03 -9.71026439e-03 ... -5.44630317e-03\n",
      "    5.43719064e-03 -5.55753428e-03]\n",
      "  [-3.30927270e-03  1.57625321e-03 -8.98024812e-03 ... -5.98474871e-03\n",
      "    7.13598216e-04  7.13733432e-04]]\n",
      "\n",
      " [[-2.91588949e-03  6.01178454e-03  3.43859690e-04 ... -5.37471520e-03\n",
      "   -1.00265583e-03 -5.30809304e-03]\n",
      "  [-3.85216717e-03  3.93746421e-04 -1.82530654e-04 ... -5.37396222e-03\n",
      "   -2.92196684e-03 -5.83638088e-04]\n",
      "  [-2.32593599e-03  2.13740813e-03 -2.86279060e-03 ... -1.22863508e-03\n",
      "    3.37679661e-03 -6.10278221e-04]\n",
      "  ...\n",
      "  [-3.11694480e-03  5.14595537e-03 -2.28118827e-03 ...  6.65866246e-05\n",
      "    9.44596156e-03 -8.83727707e-03]\n",
      "  [-4.21013450e-03  3.86403617e-03  3.39526986e-03 ... -4.05738503e-03\n",
      "    6.28101686e-03 -1.10944174e-02]\n",
      "  [-3.82687291e-03  4.28637769e-03 -8.18156230e-04 ... -7.01517286e-03\n",
      "    7.54145347e-03 -8.92771594e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-3.48714599e-03  6.61939522e-03  4.29616496e-03 ...  1.90475874e-03\n",
      "    3.00394633e-04 -3.26990150e-03]\n",
      "  [-7.23656500e-03  5.72608691e-03  3.18565662e-03 ...  3.53708444e-03\n",
      "    2.05602846e-05 -2.83568073e-03]\n",
      "  [-5.53628057e-03  7.89301936e-03 -2.88286782e-03 ...  6.64881256e-04\n",
      "   -1.84230902e-03 -6.52952632e-03]\n",
      "  ...\n",
      "  [-2.24436633e-04 -2.08946783e-03 -3.97142628e-03 ... -8.41387920e-03\n",
      "    9.28998832e-03 -7.16462499e-03]\n",
      "  [ 1.85706862e-03 -2.53757765e-03 -1.20895752e-03 ... -3.72168724e-03\n",
      "    1.15445452e-02 -1.39917266e-02]\n",
      "  [-7.06699211e-07 -7.94076594e-04 -4.66421153e-03 ... -7.62196723e-05\n",
      "    1.35259340e-02 -1.00858230e-02]]\n",
      "\n",
      " [[-5.88218682e-04  1.36934477e-03 -1.46359991e-04 ...  1.44870626e-03\n",
      "   -7.65385863e-04 -1.83485937e-03]\n",
      "  [-3.68590094e-03 -3.92734306e-03 -5.46615745e-04 ... -1.28346717e-03\n",
      "   -2.19731499e-03  1.92985521e-03]\n",
      "  [-4.23647836e-03  9.05866851e-04 -5.96745778e-03 ... -3.30818119e-03\n",
      "   -3.23009957e-03 -3.05945054e-03]\n",
      "  ...\n",
      "  [ 1.14906172e-04  1.94613182e-03 -5.92282647e-03 ... -2.84173479e-03\n",
      "    9.02220327e-03 -1.04459925e-02]\n",
      "  [-2.44088587e-03  1.96809345e-03 -7.72516755e-03 ... -2.82409310e-04\n",
      "    1.11247646e-02 -7.38602737e-03]\n",
      "  [-1.08522933e-03 -9.21000028e-05 -3.51860444e-03 ...  1.41150923e-03\n",
      "    1.29335849e-02 -1.36353718e-02]]\n",
      "\n",
      " [[ 1.13420701e-03  9.08431772e-04 -2.94142449e-03 ... -2.29606964e-03\n",
      "    2.45451462e-03 -2.10238784e-03]\n",
      "  [ 4.34693322e-03 -8.82583437e-04 -6.52504293e-03 ...  9.57041688e-04\n",
      "   -9.61742830e-04 -5.09234285e-03]\n",
      "  [ 8.84667563e-04 -5.19117061e-03 -5.48449671e-03 ... -2.83486792e-04\n",
      "   -2.89861206e-03 -3.95975658e-05]\n",
      "  ...\n",
      "  [-3.33652482e-04  7.89551705e-04 -8.64944141e-03 ... -5.24911517e-03\n",
      "   -5.28911315e-03 -3.24698444e-03]\n",
      "  [ 8.01155996e-03 -1.94523344e-03 -7.45604420e-03 ... -3.83584085e-03\n",
      "   -1.02887545e-02 -4.26916545e-03]\n",
      "  [ 1.46083615e-03  2.22947937e-03 -1.00219054e-02 ... -6.18341053e-03\n",
      "   -6.86109252e-03 -5.13862446e-03]]], shape=(64, 100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(len(example_batch_predictions))\n",
    "print(example_batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "tf.Tensor(\n",
      "[[ 0.00113421  0.00090843 -0.00294142 ... -0.00229607  0.00245451\n",
      "  -0.00210239]\n",
      " [ 0.01205073 -0.00098554 -0.0021155  ... -0.00024122 -0.00553663\n",
      "  -0.0019568 ]\n",
      " [ 0.01032698  0.00205006 -0.00527716 ...  0.00319401  0.00059211\n",
      "  -0.00149708]\n",
      " ...\n",
      " [ 0.00478191  0.00579813  0.00560208 ... -0.0106012   0.00512999\n",
      "  -0.00486678]\n",
      " [ 0.00394266  0.00698575  0.00076236 ... -0.00614349  0.00928916\n",
      "  -0.00248015]\n",
      " [ 0.00524543  0.00592864 -0.00031969 ... -0.0049257   0.01084146\n",
      "  -0.00025732]], shape=(100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Examine one prediction\n",
    "pred = example_batch_predictions[0]\n",
    "print(len(pred))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "tf.Tensor(\n",
      "[ 1.1342070e-03  9.0843177e-04 -2.9414245e-03 -3.0524228e-03\n",
      " -3.5438510e-03 -3.0541301e-03 -2.9473910e-03 -1.4797676e-03\n",
      "  3.8835178e-03  4.1887634e-03 -1.9365759e-04  3.7354336e-03\n",
      " -1.4550934e-03 -1.0582129e-03  3.5190010e-03  4.1864361e-03\n",
      " -2.5304079e-03 -2.0925761e-03 -3.5008551e-03  2.9958473e-03\n",
      "  1.6193232e-03 -1.7115383e-03  1.3750441e-03 -3.8149231e-03\n",
      " -2.1390663e-03 -1.4467712e-03 -3.6224970e-03  8.1986813e-03\n",
      " -2.6555749e-04 -2.8163185e-03  1.3159646e-05  9.2630193e-04\n",
      "  1.6621489e-03  4.2922446e-05  3.1706186e-03  6.4774007e-03\n",
      "  2.4956942e-03 -1.5643283e-03 -6.8764407e-03 -3.0439172e-03\n",
      "  2.0888292e-03  4.3299990e-03  1.3253761e-03  4.9753534e-03\n",
      "  4.5648045e-03  5.4711271e-03  3.0770786e-03  3.0528968e-03\n",
      "  2.1827032e-03 -7.9867970e-03 -1.7298812e-03  4.5429971e-03\n",
      "  5.5826786e-03  3.3996128e-03 -3.0020189e-03  2.4906220e-03\n",
      "  5.4657166e-03 -5.5148907e-04 -1.2552382e-04  5.7934155e-03\n",
      "  3.8018945e-04  3.2390123e-03 -2.2960696e-03  2.4545146e-03\n",
      " -2.1023878e-03], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# a prediction for the first timestep\n",
    "time_pred = pred[0]\n",
    "print(len(time_pred))\n",
    "print(time_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the probability of the *next* character for the first timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\".W$t!mHWmeRjtBAIjynHMRw\\n3YZ?Ty?S'HJ'tvQAVx.B.B,bUvsJs!cdFR'VixmBRHWwlEwrhiubu$CIcvqDJhjAXfnn:cQJuLyi\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probability)\n",
    "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
    "\n",
    "# then reshape the array and convert all the integers to numbers to see the actual characters\n",
    "sampled_indices = np.reshape(sampled_indices,(1,-1))[0]\n",
    "predicted_chars = int_to_text(sampled_indices)\n",
    "\n",
    "predicted_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = \"./training_checkpoints\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix,\n",
    "    save_weights_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 194s 1s/step - loss: 2.5960\n",
      "Epoch 2/5\n",
      "172/172 [==============================] - 192s 1s/step - loss: 1.8877\n",
      "Epoch 3/5\n",
      "172/172 [==============================] - 188s 1s/step - loss: 1.6372\n",
      "Epoch 4/5\n",
      "172/172 [==============================] - 186s 1s/step - loss: 1.5041\n",
      "Epoch 5/5\n",
      "172/172 [==============================] - 185s 1s/step - loss: 1.4242\n"
     ]
    }
   ],
   "source": [
    "# This took 15 mins, with more epochs the model becomes much better!\n",
    "history = model.fit(data, epochs=5, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model\n",
    "Rebuild the model from a checkpoint using a batch_size of 1 so that we can feed one piece of text to the model and have it make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load any checkpoint by specifying the exact file to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model.load_weights(latest_checkpoint)\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Number of characters to generate\n",
    "    num_generate = 800\n",
    "\n",
    "    # Convert start string to numbers (vectorizing)\n",
    "   # Convert start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    # Here begins the temperature setup and the reset of the model state\n",
    "    temp = 1.0\n",
    "    model.reset_states()\n",
    "\n",
    "    for _ in range(num_generate):\n",
    "        # This is where predictions are made\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # Here the predictions are divided by the temperature and a random choice is made\n",
    "        predictions = predictions / temp\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Prepare the predicted_id for the next round of prediction\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        # And here the predicted character is added to the generated text\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Victoria! now thus you not peagled fall,\n",
      "Ternater from eleaves wak, my father's souly!\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "A Rabenhal, at these fore Cariall'd thoughtship's sensw,\n",
      "For I moytal sleep pritaye under: day,\n",
      "That are both your dopsor traits: stay it\n",
      "poss'station, hot do boild thy almoty worse:\n",
      "Here's my old hoaring that Ramiot of yourself.\n",
      "then, loth amesitoon Pet up,\n",
      "My man tolpy hesold no whise,\n",
      "But when these norm is basely by no\n",
      "man? How thou hasits, away not spuding an your hemp,\n",
      "For vain cannot again; you wert from the king,\n",
      "That you brother like perdfulms stands, pritter, Demember,\n",
      "It scape against their squeless of these you speding enemy.\n",
      "\n",
      "POLIXENES:\n",
      "You pleased thempell:\n",
      "Then strengthing upon your galous, of cousins may\n",
      "Therefore me a woo, gry sovereign:\n",
      "Let's to so best I go to off,\n",
      "Comstakene\n"
     ]
    }
   ],
   "source": [
    "inp = input(\"Type a starting string: \")\n",
    "print(generate_text(model, inp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
